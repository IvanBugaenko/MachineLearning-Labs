INFO = r"""
# Добро пожаловать в приложение для предсказывания опасности ближайших объектов к земле!

Представим, что вы каким-то образом заполучили данные о каком-то космическом объекте; совершенно штатная ситуация, которая никого не удивит. Должен возникнуть закономерный вопрос - уничтожит ли этот объект Землю, или нет? 

Кто знает, быть может, эти **данные** вместе с **моделями машинного обучения** помогут уберечь нашу любимую планету от гибели...

![BOOOM](https://img2.rudalle.ru/images/89/f1/41/89f141516a1945fcbebd18799b16572e_00000.jpg)

Но какие это должны быть данные? Не любая же информация поможет решить столь сложную задачу, поэтому давайте разбираться!

## Описание входных данных (объекты)

Данные, конечно, - это хорошо, но правильные данные - еще лучше! Чтобы получить достоверное (или не очень) предсказание, разберемся, в чем заключается смысл данных, и какие объекты нужно подавать на вход модели машинного обучения:

|  Название столбца  | Описание                                   |
| ------------------ | ------------------------------------------ |
| est_diameter_min   | Минимальная оценка диаметра объекта        |
| est_diameter_max   | Максимальная оценка диаметра объекта       |
| relative_velocity  | Относительная скорость объекта             |
| miss_distance      | Пропущенное расстояние от объекта до Земли |
| absolute_magnitude | Абсолютная величина объекта                |


## Описание выходных данных (предсказания)

Предсказываться будет значение $hazardous \in $ {$0, 1$} - опасность объекта (0, если объект не представляет опасности, и 1, если представляет).

Таким образом, решается задача бинарной классификации.

## Выбранные модели машинного обучения

А теперь пора представить наших героев, или антигероев... модели машинного обучения!

В данном приложении используются 3 вида моделей: 
- линейные (KNN);
- ансамбли (BaggingClassifier);
- нейронные сети (многослойная полносвязная НС прямого распространения).

Познакомимся с ними поближе!

### KNN

KNN, или же метод k-ближайших соседей, являетмя одним из самых простых, но в то же время эффективных классификаторов. Он исходит из так называемой "гипотезы компактности", которая гласит о том, что объекты одного класса находятся близко к друг другу, а объекты разных классов, навпротив, - далеко друг от друга. 

Когда мы говорим "близко" или "далеко", то имеем ввиду значение функции-расстояния $\rho(x^1, x^2)$, то есть такой функции, которая показывает меру близости объектов $x^1$ и $x^2$ из нашего признакового пространства $X$.  

Функция-расстояния должна удовлетворять трем условиям:

1. $\rho(x, x) = 0$;
2. $\rho(x^1, x^2) = \rho(x^2, x^1)$;
3. $\rho(x^1, x^3) < \rho(x^1, x^2) + \rho(x^2, x^3)$.

Но все же вернемся к KNN. Задачется параметр $k$ - количество ближайших соседей. Когда нам поступает объект для классификации, мы смотрим расстояние от этого объекта до каждого из обучающей выборки. Выбираем из этих расстояний $k$ наименьших -- самые близкие объекты к нашему. И определяем, каких меток класса больше среди тех самых отобранных "соседей". Максимальная метка и будет нашим ответом.

![knn](https://www.newtechdojo.com/wp-content/uploads/2020/06/KNN-1.gif)

### BaggingClassifier

А вот и второй класс алгоритмов - ансамбли! И нет, мы будем говорить не о танцах и плясках, а об очень серьезной вещи - использовании множества одинаковых алгоритмов для решения одной задачи!

Спросите, как такое возможно? Ответ прост - BaggingClassifier. Посмотрим более подробно на то, как он устроен и с чем его едят (ни в коем случае не рекомендуем употреблять в пищу свой классификатор, это может кончиться плохо...).

Как уже было сказано, данный вид ансамблевых алгоритмов использует $n$ одинаковых моделей, предварительно настроенных, которые задаются в качестве гиперпараметра. Но возникает законный вопрос - как обучать эти модели, ведь $n$ одинаковых решений выглядит крайне странно...

И на это есть законное обоснование! При помощи техники бутстрапирования из тренировочной выборки размерности $l \times d$ случайно выбирается $l$ объектов с повторением (то есть объекты могут повторяться в подвыборке), и такая подвыборка генерируется для кажой модели внутри BaggingClassifier.

В результате, когда нам приходят данные для предсказания, мы передаем их в каждую обученную на "разных" данных модель и их результат усредняем (либо берем среднее, если задача регрессии, либо берем моду, если задача классификации). ГОЛОСОВАНИЕ!

![](https://media.geeksforgeeks.org/wp-content/uploads/20210707140912/Bagging.png)

### Нейронная сеть (многослойная полносвязная НС прямого распространения)

И гвоздь нашей программы - многослойная полносявзная нейронная сеть прямого распространения! 

Об этом товарище можно говорить долго, много и упорно, но мы сфокусируемся на том, что, глобально, такой вид нейронных сетей представляет из себя последовательное матричное умножение исходной матрицы признаков на матрицу коэффмицментов слоя (матрица связи каждого нейрона текущего слоя с каждым нейроном следующего), добавляя матрицу смещений и функцию активации к получившемуся результату. И так делается до тех пор, пока мы не пройдем все слои (выход предыдущего слоя - это вход текущего).

Можно рассматривать НС как вычислительный граф, и тогда все становится на свои места.

![НС](https://neerc.ifmo.ru/wiki/images/9/97/Single-layer-neural-net-scheme.png)
        
"""