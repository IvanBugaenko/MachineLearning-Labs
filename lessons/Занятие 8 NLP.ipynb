{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rFIgOM6i3Al"
      },
      "source": [
        "# Лабораторная работа 8. Обработка естественого языка. Начало"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDtmLvThDHOv"
      },
      "source": [
        "Обработка естественного языка (NLP) – это технология машинного обучения, которая дает компьютерам возможность интерпретировать, манипулировать и понимать человеческий язык. Сегодня организации имеют большие объемы голосовых и текстовых данных из различных каналов связи, таких как электронные письма, текстовые сообщения, новостные ленты социальных сетей, видео, аудио и многое другое. Они используют программное обеспечение NLP для автоматической обработки этих данных, анализа намерений или настроений в сообщении и реагирования на человеческое общение в режиме реального времени.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZULRBeHwDPK6"
      },
      "source": [
        "Обработка естественного языка сочетает в себе компьютерную лингвистику, машинное обучение и модели глубокого обучения для обработки человеческого языка.\n",
        "\n",
        "**Компьютерная лингвистика**\n",
        "Компьютерная лингвистика – это наука о понимании и построении моделей человеческого языка с помощью компьютеров и программных инструментов. Исследователи используют методы компьютерной лингвистики, такие как синтаксический и семантический анализ, для создания платформ, помогающих машинам понимать разговорный человеческий язык. Такие инструменты, как переводчики языков, синтезаторы текста в речь и программное обеспечение для распознавания речи, основаны на компьютерной лингвистике. \n",
        "\n",
        "**Машинное обучение**\n",
        "Машинное обучение – это технология, которая обучает компьютер с помощью выборочных данных для повышения его эффективности. Человеческий язык имеет несколько особенностей, таких как сарказм, метафоры, вариации в структуре предложений, а также исключения из грамматики и употребления, на изучение которых у людей уходят годы. Программисты используют методы машинного обучения, чтобы научить приложения NLP распознавать и точно понимать эти функции с самого начала.\n",
        "\n",
        "**Глубокое обучение**\n",
        "Глубокое обучение – это особая область машинного обучения, которая учит компьютеры учиться и мыслить как люди. Это включает нейросеть, состоящую из узлов обработки данных, напоминающих операции человеческого мозга. С помощью глубокого обучения компьютеры распознают, классифицируют и сопоставляют сложные закономерности во входных данных.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeR8-yebDVdY"
      },
      "source": [
        "## Этапы обработки\n",
        "В NLP используются методы предварительной обработки, такие как:\n",
        "1. токенизация, \n",
        "2. стемминг, лемматизация \n",
        "3. удаление стоп-слов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHhgJ-tUJfx4"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErMWKmzhJV8j",
        "outputId": "47f806a5-128c-4585-bee5-c793a33e9291"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token 0: I\n",
            "Token 1: love\n",
            "Token 2: coding\n",
            "Token 3: and\n",
            "Token 4: writing\n"
          ]
        }
      ],
      "source": [
        "## tokenizing a piecen of text\n",
        "doc = \"I love coding and writing\"\n",
        "for i, w in enumerate(doc.split(\" \")):\n",
        "    print(\"Token \" + str(i) + \": \" + w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LICJKScCQgcw"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqtsSWbNQnXw",
        "outputId": "c93c199e-77b2-4039-dab1-53052e9827ed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[I, m, student]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "doc2 = nlp('Im student')\n",
        "[token for token in doc2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcXF43GnTCIs"
      },
      "source": [
        "## Удаление стоп-слов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUpmCrvrS7G-",
        "outputId": "3620f483-bf77-44c9-943f-4b002890694a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "set"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "my_stopwords = nlp.Defaults.stop_words\n",
        "type(my_stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBArOjUPT2AV"
      },
      "outputs": [],
      "source": [
        "my_stopwords.add('f*ck')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63WNYQGvUH5b",
        "outputId": "915c3b93-da50-463c-a3a3-fa40e7caee75"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[love]"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# кто помнит Майка Чумакова, тот поймет\n",
        "doc = nlp('I love f*ck you')\n",
        "[token for token in doc if token.lower_ not in my_stopwords]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2-Mv75YJhjI"
      },
      "source": [
        "## Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxqRXmzcJmnm",
        "outputId": "7a29b1a9-ffdc-49d9-b1dd-6415880881c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I => I\n",
            "love => love\n",
            "coding => code\n",
            "and => and\n",
            "writing => write\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['I', 'love', 'code', 'and', 'write']"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## import the libraries\n",
        "import spacy\n",
        "from spacy.lookups import Lookups\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "## lemmatization\n",
        "# токенизация (получение объекта doc)\n",
        "doc = nlp(u'I love coding and writing')\n",
        "# лемматизация\n",
        "for word in doc:\n",
        "    print(word.text, \"=>\", word.lemma_)\n",
        "data_lematized = [word.lemma_ for word in doc]\n",
        "data_lematized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iGjAE1qJu-O"
      },
      "source": [
        "## Stemming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xMFI52qQkxz"
      },
      "source": [
        "# Word embedding -- векторное представление слов \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JxrYbiDRWXB"
      },
      "source": [
        "## Bag of words\n",
        "\n",
        "Рассмотрим самый простой способ приведения текста к набору чисел. Для каждого слова посчитаем, как часто оно встречается в тексте. Результаты запишем в таблицу. Строки будут представлять тексты, столбцы -- слова. Если на пересечении строки с столбца стоит число 5, значит данное слово встретилось в данном тексте 5 раз. В большинстве ячеек будут нули. Поэтому хранить это всё удобнее в виде разреженных матриц (т.е. хранить только ненулевые значения).\n",
        "\n",
        "Таким образом, при построении \"мешка слов\" можно выделить следующие действия:\n",
        "\n",
        "1. Токенизация.\n",
        "\n",
        "2. Построение словаря: собираем все слова, которые встречались в текстах и пронумеровываем их (по алфавиту, например).\n",
        "\n",
        "3. Построение разреженной матрицы. В sklearn алгоритм приведения текста в bag-of-words реализован в виде класса CountVectorizer. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeWNrMvmRVjG",
        "outputId": "3567dc4a-67a6-4bda-f0e6-2c94f53b783c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3, 48)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "count_vectorizer = CountVectorizer()\n",
        "texts = [\"Великолепный сериал, который поможет успокоить нервы при любых стрессах и просто скрасит серые будни\",\n",
        "         \"Пожалуй, если бы я посмотрел только первые пару сезонов этого сериала, я бы с легкой руки написал ему положительную рецензию\",\n",
        "         \"В общем, если создатели этого сериала не вернут всё на круги своя, то рейтинги следующих сезонов будут становится все ниже и ниже, а зрительская аудитория будет все меньше и меньше.\"]\n",
        "\n",
        "bow = count_vectorizer.fit_transform(texts)\n",
        "bow.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d111Us5hSxzL"
      },
      "source": [
        "Результат содержит 3 строки (для 3 текстов) и 48 столбцов (для 48 разных слов). Посмотрим словарь:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1io42iauSwTb",
        "outputId": "0165677e-3b8e-4736-82e8-9c7093ea924b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'великолепный': 5,\n",
              " 'сериал': 36,\n",
              " 'который': 12,\n",
              " 'поможет': 27,\n",
              " 'успокоить': 46,\n",
              " 'нервы': 20,\n",
              " 'при': 29,\n",
              " 'любых': 15,\n",
              " 'стрессах': 43,\n",
              " 'просто': 30,\n",
              " 'скрасит': 39,\n",
              " 'серые': 38,\n",
              " 'будни': 2,\n",
              " 'пожалуй': 25,\n",
              " 'если': 10,\n",
              " 'бы': 4,\n",
              " 'посмотрел': 28,\n",
              " 'только': 45,\n",
              " 'первые': 24,\n",
              " 'пару': 23,\n",
              " 'сезонов': 35,\n",
              " 'этого': 47,\n",
              " 'сериала': 37,\n",
              " 'легкой': 14,\n",
              " 'руки': 33,\n",
              " 'написал': 18,\n",
              " 'ему': 9,\n",
              " 'положительную': 26,\n",
              " 'рецензию': 32,\n",
              " 'общем': 22,\n",
              " 'создатели': 41,\n",
              " 'не': 19,\n",
              " 'вернут': 6,\n",
              " 'всё': 8,\n",
              " 'на': 17,\n",
              " 'круги': 13,\n",
              " 'своя': 34,\n",
              " 'то': 44,\n",
              " 'рейтинги': 31,\n",
              " 'следующих': 40,\n",
              " 'будут': 3,\n",
              " 'становится': 42,\n",
              " 'все': 7,\n",
              " 'ниже': 21,\n",
              " 'зрительская': 11,\n",
              " 'аудитория': 0,\n",
              " 'будет': 1,\n",
              " 'меньше': 16}"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "count_vectorizer.vocabulary_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQVMFmKSTBfD",
        "outputId": "706ccbd9-2219-4696-f731-0faa3273c21a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "matrix([[0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,\n",
              "         0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,\n",
              "         0, 1, 0, 0, 1, 0],\n",
              "        [0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
              "         0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,\n",
              "         0, 0, 0, 1, 0, 1],\n",
              "        [1, 1, 0, 1, 0, 0, 1, 2, 1, 0, 1, 1, 0, 1, 0, 0, 2, 1, 0, 1, 0,\n",
              "         2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
              "         1, 0, 1, 0, 0, 1]])"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bow.todense()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mECQhd7uTFqs"
      },
      "source": [
        "Как видим, ни стемминга, ни лемматизации по умолчанию не производится.\n",
        "Поэтому для уменьшения размерности данной матрицы требуется производить лемматизацию/стемминг и удаления стоп-слов. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEP3drzOTQgu"
      },
      "source": [
        "**Параметр min_df**\n",
        "\n",
        "Помимо выше озвученных есть и другие способы отсечения лишнего. Например, можно откидывать слова, которые встречаются слишком редко, с помощью параметра min_df. Установив min_df=2 мы откинем, все слова, которые встречаются менее, чем в 2 документах."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUrVKE9eVvvK",
        "outputId": "152cbccc-d531-4ebd-bb37-60d48e689f4f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'если': 0, 'сезонов': 1, 'этого': 3, 'сериала': 2}"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "count_vectorizer = CountVectorizer(min_df=2)\n",
        "bow = count_vectorizer.fit_transform(texts)\n",
        "count_vectorizer.vocabulary_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfA4vT56V_F5"
      },
      "source": [
        "**Биграммы, триграммы, n-граммы**\n",
        "\n",
        "По умолчанию bag-of-words (как следует из названия) представляет собой просто мешок слов. То есть для него предложения \"It's not good, it's bad!\" и \"It's not bad, it's good!\" абсолютно эквивалентны. Понятно, что при этом теряется много информации. Можно рассматривать не только отдельные слова, а последовательности длиной из 2 слов (биграммы), из 3 слов (триграммы) или в общем случае из n слов (n-граммы). На практике обычно задаётся диапазон от 1 до n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCpfC36oWBxB",
        "outputId": "1ff896b1-4430-46c3-ddf1-589759de6322"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'если': 0, 'сезонов': 1, 'этого': 3, 'сериала': 2, 'этого сериала': 4}\n",
            "[[1 1 0 0 0]]\n",
            "[[0 2 1 2 1]]\n"
          ]
        }
      ],
      "source": [
        "count_vectorizer = CountVectorizer(ngram_range=(1,2), min_df=2)\n",
        "bow = count_vectorizer.fit_transform(texts)\n",
        "print(count_vectorizer.vocabulary_)\n",
        "\n",
        "print(count_vectorizer.transform(['Если несколько сезонов']).todense())\n",
        "print(count_vectorizer.transform(['Этого сериала этого сезонов сезонов']).todense())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riLA3FPAWRzP"
      },
      "source": [
        "**Ограничение количества признаков**\n",
        "\n",
        "Понятно, что с ростом n количество выделенных n-грамм быстро растёт. Для ограничения количества признаков можно использовать параметр max_features. В этом случае будет создано не более max_features признаков (будут выбраны самые часто встречающиеся слова и последовательности слов). Например:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64h04sKlWUH1",
        "outputId": "adeb72dd-59e3-4b34-9661-399c649e5a37"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'поможет': 7,\n",
              " 'при': 11,\n",
              " 'просто': 13,\n",
              " 'поможет успокоить': 8,\n",
              " 'при любых': 12,\n",
              " 'просто скрасит': 14,\n",
              " 'если': 2,\n",
              " 'бы': 0,\n",
              " 'посмотрел': 9,\n",
              " 'сезонов': 21,\n",
              " 'этого': 23,\n",
              " 'сериала': 22,\n",
              " 'руки': 17,\n",
              " 'положительную': 5,\n",
              " 'рецензию': 16,\n",
              " 'посмотрел только': 10,\n",
              " 'этого сериала': 24,\n",
              " 'руки написал': 18,\n",
              " 'положительную рецензию': 6,\n",
              " 'своя': 19,\n",
              " 'все': 1,\n",
              " 'ниже': 4,\n",
              " 'меньше': 3,\n",
              " 'своя то': 20,\n",
              " 'рейтинги следующих': 15}"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "count_vectorizer = CountVectorizer(ngram_range=(1,2), max_features=25)\n",
        "bow = count_vectorizer.fit_transform(texts)\n",
        "count_vectorizer.vocabulary_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoGGIlJ1Wf5Z"
      },
      "source": [
        "## TF-IDF\n",
        "\n",
        "У подхода bag-of-words есть существенный недостаток. Если слово встречается 5 раз в конкретном документе, но и в других документах тоже встречается часто, то его наличие в документе не особо-то о чём-то говорит. Если же слово 5 раз встречается в конкретном документе, но в других документах встречается редко, то его наличие (да ещё и многократное) позволяет хорошо отличать этот документ от других. Однако с точки зрения bag-of-words различий не будет: в обеих ячейках будет просто число 5.\n",
        "\n",
        "Отчасти это решается исключением стоп-слов (и слишком часто встречающихся слов), но лишь отчасти. Другой идеей является отмасштабировать получившуюся таблицу с учётом \"редкости\" слова в наборе документов (т.е. с учётом информативности слова).\n",
        "\n",
        "$tfidf=tf∗idf$\n",
        "\n",
        "$idf=\\log\\frac{(N+1)}{(Nw+1)}+1$\n",
        "\n",
        "Здесь tf это частота слова в тексте (то же самое, что в bag of words), N - общее число документов, Nw - число документов, содержащих данное слово.\n",
        "\n",
        "То есть для каждого слова считается отношение общего количества документов к количеству документов, содержащих данное слово (для частых слов оно будет ближе к 1, для редких слов оно будет стремиться к числу, равному количеству документов), и на логарифм от этого числа умножается исходное значение bag-of-words (к числителю и знаменателю прибавляется единичка, чтобы не делить на 0, и к логарифму тоже прибавляется единичка, но это уже технические детали). После этого в sklearn ещё проводится L2-нормализация каждой строки.\n",
        "\n",
        "В sklearn есть класс для поддержки TF-IDF: TfidfVectorizer, рассмотрим его."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ioHGsp6AYkM3",
        "outputId": "36054a74-4db2-4c49-daa7-475e20493615"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'великолепный': 5,\n",
              " 'сериал': 36,\n",
              " 'который': 12,\n",
              " 'поможет': 27,\n",
              " 'успокоить': 46,\n",
              " 'нервы': 20,\n",
              " 'при': 29,\n",
              " 'любых': 15,\n",
              " 'стрессах': 43,\n",
              " 'просто': 30,\n",
              " 'скрасит': 39,\n",
              " 'серые': 38,\n",
              " 'будни': 2,\n",
              " 'пожалуй': 25,\n",
              " 'если': 10,\n",
              " 'бы': 4,\n",
              " 'посмотрел': 28,\n",
              " 'только': 45,\n",
              " 'первые': 24,\n",
              " 'пару': 23,\n",
              " 'сезонов': 35,\n",
              " 'этого': 47,\n",
              " 'сериала': 37,\n",
              " 'легкой': 14,\n",
              " 'руки': 33,\n",
              " 'написал': 18,\n",
              " 'ему': 9,\n",
              " 'положительную': 26,\n",
              " 'рецензию': 32,\n",
              " 'общем': 22,\n",
              " 'создатели': 41,\n",
              " 'не': 19,\n",
              " 'вернут': 6,\n",
              " 'всё': 8,\n",
              " 'на': 17,\n",
              " 'круги': 13,\n",
              " 'своя': 34,\n",
              " 'то': 44,\n",
              " 'рейтинги': 31,\n",
              " 'следующих': 40,\n",
              " 'будут': 3,\n",
              " 'становится': 42,\n",
              " 'все': 7,\n",
              " 'ниже': 21,\n",
              " 'зрительская': 11,\n",
              " 'аудитория': 0,\n",
              " 'будет': 1,\n",
              " 'меньше': 16}"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf = tfidf_vectorizer.fit_transform(texts)\n",
        "tfidf_vectorizer.vocabulary_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmKX6oGcYqSm"
      },
      "source": [
        "Словарь содержит те же 48 значений, которые были бы и для CountVectorizer. Но значения в таблице другие:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cpgf0X77YmNn",
        "outputId": "b29299b0-f4be-4aa3-d497-d6652d529b4b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "matrix([[0.        , 0.        , 0.2773501 , 0.        , 0.        ,\n",
              "         0.2773501 , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.2773501 , 0.        , 0.        ,\n",
              "         0.2773501 , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.2773501 , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.2773501 , 0.        , 0.2773501 ,\n",
              "         0.2773501 , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.2773501 , 0.        , 0.2773501 , 0.2773501 ,\n",
              "         0.        , 0.        , 0.        , 0.2773501 , 0.        ,\n",
              "         0.        , 0.2773501 , 0.        ],\n",
              "        [0.        , 0.        , 0.        , 0.        , 0.48065817,\n",
              "         0.        , 0.        , 0.        , 0.        , 0.24032909,\n",
              "         0.18277647, 0.        , 0.        , 0.        , 0.24032909,\n",
              "         0.        , 0.        , 0.        , 0.24032909, 0.        ,\n",
              "         0.        , 0.        , 0.        , 0.24032909, 0.24032909,\n",
              "         0.24032909, 0.24032909, 0.        , 0.24032909, 0.        ,\n",
              "         0.        , 0.        , 0.24032909, 0.24032909, 0.        ,\n",
              "         0.18277647, 0.        , 0.18277647, 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.24032909, 0.        , 0.18277647],\n",
              "        [0.18162735, 0.18162735, 0.        , 0.18162735, 0.        ,\n",
              "         0.        , 0.18162735, 0.36325471, 0.18162735, 0.        ,\n",
              "         0.13813228, 0.18162735, 0.        , 0.18162735, 0.        ,\n",
              "         0.        , 0.36325471, 0.18162735, 0.        , 0.18162735,\n",
              "         0.        , 0.36325471, 0.18162735, 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.18162735, 0.        , 0.        , 0.18162735,\n",
              "         0.13813228, 0.        , 0.13813228, 0.        , 0.        ,\n",
              "         0.18162735, 0.18162735, 0.18162735, 0.        , 0.18162735,\n",
              "         0.        , 0.        , 0.13813228]])"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tfidf.todense()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mODMPXvhZBbo"
      },
      "source": [
        "Ненулевые значения находятся на тех же местах, но отмасштабированы в зависимости от частоты слов."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1k2kwuOZFj3"
      },
      "source": [
        "**Параметр sublinear_tf**\n",
        "\n",
        "Большая часть параметров у CountVectorizer и TfidfVectorizer одинакова. Но у TfidfVectorizer есть один важный дополнительный параметр.\n",
        "\n",
        "Как видно из формулы tfidf = tf * idf, если слово будет встречаться не один, а два раза, то tfidf вырастет в два раза. Если слово будет встречаться не один, а 10 раз, то tfidf вырастет почти в 10 раз. В качестве примера добавим в третью строку ещё пару слов меньше"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0F-HviBYvoA",
        "outputId": "a0366dc2-7c7b-4109-cc10-d8c799f08963"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "matrix([[0.15373049, 0.15373049, 0.        , 0.15373049, 0.        ,\n",
              "         0.        , 0.15373049, 0.30746099, 0.15373049, 0.        ,\n",
              "         0.116916  , 0.15373049, 0.        , 0.15373049, 0.        ,\n",
              "         0.        , 0.61492198, 0.15373049, 0.        , 0.15373049,\n",
              "         0.        , 0.30746099, 0.15373049, 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.15373049, 0.        , 0.        , 0.15373049,\n",
              "         0.116916  , 0.        , 0.116916  , 0.        , 0.        ,\n",
              "         0.15373049, 0.15373049, 0.15373049, 0.        , 0.15373049,\n",
              "         0.        , 0.        , 0.116916  ]])"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "texts = [\"Великолепный сериал, который поможет успокоить нервы при любых стрессах и просто скрасит серые будни\",\n",
        "         \"Пожалуй, если бы я посмотрел только первые пару сезонов этого сериала, я бы с легкой руки написал ему положительную рецензию\",\n",
        "         \"В общем, если создатели этого сериала не вернут всё на круги своя, то рейтинги следующих сезонов будут становится все ниже и ниже, а зрительская аудитория будет все меньше и меньше и меньше и меньше.\"]\n",
        "TfidfVectorizer().fit_transform(texts).todense()[2]         "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfDTxTa2ZSWH"
      },
      "source": [
        "Значение tfidf слова \"меньше\" выросло с 0.36325471 до 0.61492198, а остальные упали .\n",
        "\n",
        "Вопрос - хотим ли мы таких сильных изменений. Если не хотим, то можно использовать параметр sublinear_tf=True. При его использовании вместо tf будет браться 1 + log(tf). То есть по-прежнему с ростом tf будет расти и tfidf, но уже не так радикально (и соответственно остальные значения будут уменьшаться не так быстро). Для некоторых задач это может дать прирост в качестве."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBgHVX_pZKsY",
        "outputId": "78a4d24f-7770-4fb4-9e72-16c16f9d69bc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "matrix([[0.18336592, 0.18336592, 0.        , 0.18336592, 0.        ,\n",
              "         0.        , 0.18336592, 0.31046549, 0.18336592, 0.        ,\n",
              "         0.13945451, 0.18336592, 0.        , 0.18336592, 0.        ,\n",
              "         0.        , 0.43756505, 0.18336592, 0.        , 0.18336592,\n",
              "         0.        , 0.31046549, 0.18336592, 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.18336592, 0.        , 0.        , 0.18336592,\n",
              "         0.13945451, 0.        , 0.13945451, 0.        , 0.        ,\n",
              "         0.18336592, 0.18336592, 0.18336592, 0.        , 0.18336592,\n",
              "         0.        , 0.        , 0.13945451]])"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "TfidfVectorizer(sublinear_tf=True).fit_transform(texts).todense()[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLI-fF73bUdn",
        "outputId": "1920f2e2-23a1-4e71-fb83-cdd3969b306a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(96,)"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "doc[0].vector.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGAL_G_zDupn"
      },
      "source": [
        "# LDA\n",
        "\n",
        "LDA принадлежит семейству порождающий вероятностных моделей, в которых темы представлены вероятностями появления каждого слова из заданного набора. Документы в свою очередь могут быть представлены как сочетания тем. Уникальная особенность моделей LDA состоит в том что темы не обязательно должны быть различными и слова могут встречаться в нескольких темах; это придает некоторую нечеткость определяемым темам, что может пригодиться для совладения с гибкостью языка.\n",
        "\n",
        "Для проведения тематического моделирования с помощью LDA можно использовать [sklearn.decomposition.LatentDirichletAllocation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4uSDKYwEwx3"
      },
      "outputs": [],
      "source": [
        "from time import time\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.datasets import fetch_20newsgroups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIH6LoX6E40P"
      },
      "outputs": [],
      "source": [
        "n_samples = 2000\n",
        "n_features = 1000\n",
        "n_topics = 10\n",
        "n_top_words = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_o-owByVH5zB"
      },
      "outputs": [],
      "source": [
        "def print_top_words(model, feature_names, n_top_words):\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        print(\"Topic #%d:\" % topic_idx)\n",
        "        print(\" \".join([feature_names[i]\n",
        "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0BJQUwMH6Z5",
        "outputId": "b5b32adc-6771-4e2f-f0df-5dedda4cea58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ]
        }
      ],
      "source": [
        "print(\"Loading dataset...\")\n",
        "dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n",
        "                             remove=('headers', 'footers', 'quotes'))\n",
        "data_samples = dataset.data[:n_samples]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4jL4yebIILz",
        "outputId": "4f5d5840-1795-487a-d12f-e58c3cfea426"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting tf features for LDA...\n"
          ]
        }
      ],
      "source": [
        "# Use tf (raw term count) features for LDA.\n",
        "print(\"Extracting tf features for LDA...\")\n",
        "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
        "                                max_features=n_features,\n",
        "                                stop_words='english')\n",
        "tf = tf_vectorizer.fit_transform(data_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqn03YEof36y",
        "outputId": "a9ed1935-c456-46aa-ce48-d11942d2f227"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2000, 1000)"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQftgYw8IAvw"
      },
      "outputs": [],
      "source": [
        "lda = LatentDirichletAllocation(n_components=n_topics, max_iter=20, \n",
        "                                learning_method='online',\n",
        "                                learning_offset=50.,\n",
        "                                random_state=0).fit(tf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdjMq9E9IS6K",
        "outputId": "38830090-61a6-4111-f3ea-f7fbf921e5a4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['00', '000', '10', '100', '11', '12', '128', '13', '130', '14',\n",
              "       '15', '16', '17', '18', '19', '1992', '1993', '20', '200', '21',\n",
              "       '22', '23', '24', '25', '250', '26', '27', '28', '29', '2nd', '30',\n",
              "       '300', '31', '32', '33', '34', '35', '36', '37', '38', '3d', '40',\n",
              "       '42', '43', '44', '45', '48', '49', '50', '500', '51', '55', '60',\n",
              "       '66', '70', '72', '75', '80', '800', '86', '90', '92', '93', '__',\n",
              "       'able', 'ac', 'accept', 'access', 'according', 'act', 'action',\n",
              "       'actually', 'add', 'added', 'addition', 'address',\n",
              "       'administration', 'advance', 'age', 'ago', 'agree', 'aids', 'air',\n",
              "       'al', 'allow', 'allowed', 'alt', 'america', 'american', 'amiga',\n",
              "       'analysis', 'anonymous', 'answer', 'answers', 'anti', 'anybody',\n",
              "       'apartment', 'appears', 'apple', 'application', 'applications',\n",
              "       'apply', 'appreciated', 'approach', 'appropriate', 'apr', 'april',\n",
              "       'archive', 'area', 'areas', 'aren', 'argument', 'armenia',\n",
              "       'armenian', 'armenians', 'army', 'article', 'ask', 'asked',\n",
              "       'asking', 'assume', 'atheism', 'attack', 'attacks', 'attempt',\n",
              "       'au', 'author', 'authority', 'available', 'average', 'away',\n",
              "       'azerbaijan', 'bad', 'based', 'basic', 'basically', 'begin',\n",
              "       'belief', 'believe', 'best', 'better', 'bible', 'big', 'bike',\n",
              "       'billion', 'bios', 'bit', 'black', 'block', 'blood', 'blue',\n",
              "       'board', 'bob', 'body', 'book', 'books', 'bought', 'box', 'brake',\n",
              "       'break', 'bring', 'btw', 'build', 'building', 'built', 'bus',\n",
              "       'business', 'buy', 'buying', 'ca', 'cable', 'called', 'calls',\n",
              "       'came', 'canada', 'car', 'card', 'cards', 'care', 'carry', 'cars',\n",
              "       'case', 'cases', 'cause', 'cc', 'cd', 'center', 'certain',\n",
              "       'certainly', 'chance', 'change', 'changed', 'changes', 'cheap',\n",
              "       'check', 'children', 'chip', 'choice', 'christ', 'christian',\n",
              "       'christians', 'church', 'citizens', 'city', 'claim', 'clear',\n",
              "       'clearly', 'clinton', 'clipper', 'clock', 'close', 'code', 'cold',\n",
              "       'color', 'com', 'come', 'comes', 'coming', 'command', 'comments',\n",
              "       'commercial', 'common', 'communications', 'community', 'comp',\n",
              "       'company', 'complete', 'completely', 'computer', 'condition',\n",
              "       'conference', 'congress', 'connector', 'consider', 'considered',\n",
              "       'contact', 'containing', 'contains', 'continue', 'control',\n",
              "       'controller', 'copies', 'copy', 'correct', 'cost', 'costs',\n",
              "       'couldn', 'countries', 'country', 'couple', 'course', 'court',\n",
              "       'cover', 'create', 'created', 'crime', 'crowd', 'cs', 'cubs',\n",
              "       'current', 'currently', 'cut', 'dangerous', 'data', 'database',\n",
              "       'date', 'dave', 'david', 'day', 'days', 'dc', 'dead', 'deal',\n",
              "       'death', 'dec', 'decided', 'defense', 'deleted', 'department',\n",
              "       'design', 'designed', 'details', 'developed', 'development',\n",
              "       'device', 'devices', 'did', 'didn', 'die', 'died', 'difference',\n",
              "       'different', 'difficult', 'digital', 'directly', 'directory',\n",
              "       'discussion', 'disease', 'disk', 'display', 'division', 'does',\n",
              "       'doesn', 'dog', 'doing', 'dollars', 'don', 'door', 'dos', 'dot',\n",
              "       'doubt', 'dr', 'drive', 'driver', 'drivers', 'drives', 'driving',\n",
              "       'drug', 'earlier', 'early', 'earth', 'easily', 'easy', 'edu',\n",
              "       'effect', 'effective', 'effort', 'email', 'encrypted',\n",
              "       'encryption', 'end', 'energy', 'enforcement', 'engine', 'entire',\n",
              "       'equipment', 'eric', 'error', 'especially', 'events', 'evidence',\n",
              "       'exactly', 'example', 'excellent', 'exist', 'expect', 'experience',\n",
              "       'explain', 'extra', 'face', 'fact', 'fair', 'fairly', 'faith',\n",
              "       'family', 'faq', 'far', 'fast', 'faster', 'father', 'fax',\n",
              "       'feature', 'features', 'federal', 'feel', 'field', 'figure',\n",
              "       'file', 'files', 'final', 'finally', 'fine', 'firearm', 'fit',\n",
              "       'floppy', 'flyers', 'folks', 'follow', 'following', 'food',\n",
              "       'force', 'forget', 'form', 'format', 'formats', 'free', 'freedom',\n",
              "       'friend', 'ftp', 'function', 'future', 'game', 'games', 'gas',\n",
              "       'gave', 'gay', 'general', 'generally', 'gets', 'getting', 'given',\n",
              "       'gives', 'giving', 'gm', 'goal', 'god', 'goes', 'going', 'gone',\n",
              "       'good', 'got', 'gov', 'government', 'graphics', 'great', 'greek',\n",
              "       'ground', 'group', 'groups', 'guess', 'gun', 'guns', 'guy', 'half',\n",
              "       'hand', 'happen', 'happened', 'happens', 'happy', 'hard',\n",
              "       'hardware', 'haven', 'having', 'head', 'heads', 'health', 'hear',\n",
              "       'heard', 'heart', 'heaven', 'held', 'hell', 'help', 'hi', 'high',\n",
              "       'higher', 'history', 'hit', 'hiv', 'hockey', 'hold', 'home',\n",
              "       'hope', 'hospital', 'hot', 'hours', 'house', 'hp', 'human', 'ibm',\n",
              "       'id', 'idea', 'ii', 'image', 'images', 'imagine', 'important',\n",
              "       'include', 'included', 'includes', 'including', 'increase',\n",
              "       'individual', 'info', 'information', 'input', 'inside', 'instead',\n",
              "       'insurance', 'interested', 'interesting', 'interface', 'internal',\n",
              "       'international', 'internet', 'involved', 'isn', 'israel',\n",
              "       'israeli', 'issue', 'james', 'jesus', 'jewish', 'jews', 'job',\n",
              "       'jobs', 'john', 'just', 'kept', 'key', 'keys', 'kill', 'killed',\n",
              "       'killing', 'kind', 'knew', 'know', 'knowledge', 'known', 'knows',\n",
              "       'lack', 'land', 'language', 'large', 'late', 'later', 'latest',\n",
              "       'launch', 'law', 'laws', 'leafs', 'learn', 'leave', 'led', 'left',\n",
              "       'legal', 'let', 'letter', 'level', 'library', 'license', 'life',\n",
              "       'light', 'like', 'likely', 'limited', 'line', 'lines', 'list',\n",
              "       'listen', 'little', 'live', 'lives', 'living', 'll', 'local',\n",
              "       'long', 'longer', 'look', 'looking', 'looks', 'lord', 'lost',\n",
              "       'lot', 'lots', 'love', 'low', 'luck', 'lunar', 'mac', 'machine',\n",
              "       'machines', 'magi', 'mail', 'main', 'major', 'make', 'makes',\n",
              "       'making', 'mamma', 'man', 'manager', 'manual', 'mark', 'market',\n",
              "       'marriage', 'mars', 'mary', 'mass', 'master', 'math', 'matter',\n",
              "       'maybe', 'mb', 'mean', 'meaning', 'means', 'media', 'medical',\n",
              "       'member', 'members', 'memory', 'men', 'mention', 'mentioned',\n",
              "       'message', 'middle', 'mike', 'mil', 'miles', 'military', 'million',\n",
              "       'mind', 'mission', 'mit', 'mode', 'model', 'models', 'modern',\n",
              "       'money', 'monitor', 'months', 'moon', 'moral', 'mother', 'motif',\n",
              "       'mr', 'ms', 'nasa', 'national', 'nature', 'navy', 'near',\n",
              "       'necessary', 'need', 'needed', 'needs', 'net', 'network', 'new',\n",
              "       'news', 'newsgroup', 'nhl', 'nice', 'night', 'non', 'normal',\n",
              "       'north', 'note', 'nsa', 'number', 'numbers', 'objects', 'obvious',\n",
              "       'offer', 'office', 'oh', 'oil', 'ok', 'old', 'older', 'ones',\n",
              "       'open', 'opinion', 'opinions', 'orbit', 'order', 'org',\n",
              "       'organization', 'original', 'os', 'output', 'outside', 'package',\n",
              "       'page', 'paper', 'papers', 'parents', 'particular', 'parts',\n",
              "       'party', 'pass', 'past', 'paul', 'pay', 'pc', 'people',\n",
              "       'performance', 'period', 'person', 'personal', 'peter', 'phone',\n",
              "       'pick', 'piece', 'pin', 'pittsburgh', 'place', 'places', 'plan',\n",
              "       'play', 'played', 'player', 'players', 'playing', 'plus', 'point',\n",
              "       'points', 'police', 'policy', 'political', 'population', 'port',\n",
              "       'position', 'possible', 'post', 'posted', 'posting', 'power', 'pp',\n",
              "       'present', 'president', 'press', 'pretty', 'price', 'printer',\n",
              "       'private', 'pro', 'probably', 'probe', 'probes', 'problem',\n",
              "       'problems', 'process', 'product', 'program', 'programs', 'project',\n",
              "       'prove', 'provide', 'pub', 'public', 'purpose', 'putting',\n",
              "       'quality', 'question', 'questions', 'quite', 'radio', 'ram',\n",
              "       'range', 'rate', 'rates', 'ray', 'read', 'reading', 'real',\n",
              "       'really', 'reason', 'reasonable', 'received', 'recent', 'recently',\n",
              "       'recommend', 'record', 'red', 'reference', 'regular', 'related',\n",
              "       'release', 'religion', 'religious', 'remember', 'reply', 'report',\n",
              "       'request', 'require', 'required', 'research', 'response', 'rest',\n",
              "       'result', 'results', 'return', 'right', 'rights', 'road', 'robert',\n",
              "       'role', 'rom', 'room', 'rules', 'run', 'running', 'runs', 'safety',\n",
              "       'said', 'sale', 'san', 'save', 'saw', 'say', 'saying', 'says',\n",
              "       'school', 'sci', 'science', 'scientific', 'screen', 'scsi',\n",
              "       'search', 'season', 'second', 'section', 'secure', 'security',\n",
              "       'seen', 'self', 'sell', 'send', 'sense', 'sent', 'serial',\n",
              "       'series', 'seriously', 'server', 'service', 'set', 'sex', 'sgi',\n",
              "       'shall', 'short', 'shot', 'shots', 'shows', 'shuttle', 'signal',\n",
              "       'similar', 'simple', 'simply', 'sin', 'single', 'site',\n",
              "       'situation', 'size', 'slow', 'small', 'society', 'software',\n",
              "       'solar', 'sold', 'soldiers', 'soon', 'sorry', 'sort', 'sound',\n",
              "       'sounds', 'source', 'sources', 'soviet', 'space', 'spacecraft',\n",
              "       'special', 'specific', 'specifically', 'speed', 'st', 'standard',\n",
              "       'start', 'started', 'starting', 'state', 'statement', 'states',\n",
              "       'station', 'stay', 'stop', 'story', 'street', 'strong', 'study',\n",
              "       'stuff', 'stupid', 'subject', 'suggest', 'sun', 'support',\n",
              "       'supported', 'supports', 'suppose', 'supposed', 'sure', 'surface',\n",
              "       'switch', 'systems', 'taken', 'takes', 'taking', 'talk', 'talking',\n",
              "       'tape', 'team', 'teams', 'technical', 'technology', 'tell', 'term',\n",
              "       'test', 'text', 'thank', 'thanks', 'theory', 'thing', 'things',\n",
              "       'think', 'thinking', 'thought', 'time', 'times', 'tires', 'today',\n",
              "       'told', 'took', 'toronto', 'total', 'town', 'traffic', 'transfer',\n",
              "       'tried', 'trouble', 'true', 'trust', 'truth', 'try', 'trying',\n",
              "       'turkish', 'turn', 'turned', 'tv', 'type', 'types', 'uk',\n",
              "       'understand', 'unfortunately', 'united', 'university', 'unix',\n",
              "       'unless', 'use', 'used', 'useful', 'user', 'users', 'uses',\n",
              "       'using', 'usually', 'value', 'van', 'various', 've', 'version',\n",
              "       'vga', 'video', 'view', 'volume', 'vs', 'want', 'wanted', 'wants',\n",
              "       'war', 'washington', 'wasn', 'water', 'way', 'weapon', 'weapons',\n",
              "       'week', 'weeks', 'went', 'western', 'white', 'wife', 'willing',\n",
              "       'win', 'window', 'windows', 'wish', 'woman', 'women', 'won',\n",
              "       'wonder', 'wondering', 'word', 'words', 'work', 'worked',\n",
              "       'working', 'works', 'world', 'worse', 'worth', 'wouldn', 'write',\n",
              "       'written', 'wrong', 'xfree86', 'year', 'years', 'yes', 'young'],\n",
              "      dtype=object)"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf_vectorizer.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fTjSFEehCCa",
        "outputId": "ac8663e5-e1a7-432b-d497-3bcd51c72d74"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(10, 1000)"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# матрица темы-слова\n",
        "lda.components_.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5utkTYOBh_cQ",
        "outputId": "a2640a62-2eaa-4f79-d669-4aafee4f8d40"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2000, 10)"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# матрица документы-темы\n",
        "lda.transform(tf).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtN1TO9ziLKI",
        "outputId": "8bc252e2-5a67-4b1c-f5ce-33aa1fcc98e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Topic #0:\n",
            "edu com mail send graphics ftp pub available contact version\n",
            "Topic #1:\n",
            "don like just think know ve good way people right\n",
            "Topic #2:\n",
            "think christian book atheism new pittsburgh president like game just\n",
            "Topic #3:\n",
            "drive windows disk thanks use card drives hard using software\n",
            "Topic #4:\n",
            "hiv health aids april disease research care medical information 1993\n",
            "Topic #5:\n",
            "god people does jesus law say just life don israel\n",
            "Topic #6:\n",
            "10 55 11 15 game 12 18 team 20 19\n",
            "Topic #7:\n",
            "car year new bike cars good engine just price oil\n",
            "Topic #8:\n",
            "people said didn went did know time just like took\n",
            "Topic #9:\n",
            "key space government public use law encryption section keys earth\n"
          ]
        }
      ],
      "source": [
        "print_top_words(lda, tf_vectorizer.get_feature_names_out(), 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWh582N5ZZnJ"
      },
      "source": [
        "# Задание\n",
        "Используя данные https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset\n",
        "1. Самостоятельно реализовать BoW, TF-IDF\n",
        "2. Решить задачу классификации с понижением размерности. Использовать самостоятельно реализованные модели из предыдущих ЛР.\n",
        "3. Решить задачу мягкой кластеризации (ТМ) с помощью LDA\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
